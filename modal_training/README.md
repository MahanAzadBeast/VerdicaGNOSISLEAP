# Modal MolBERT Training Deployment

## ðŸš€ Quick Setup

### 1. Install Modal
```bash
pip install modal
```

### 2. Authenticate Modal
```bash
modal token new
```

### 3. Deploy Training
```bash
cd /app/modal_training

# Single target training
modal run modal_molbert.py --target EGFR

# All targets training  
modal run modal_molbert.py --all-targets

# With progress webhook
modal run modal_molbert.py --target EGFR --webhook-url https://your-app.com/api/gpu/training-progress
```

## ðŸ“Š Modal GPU Options & Performance

| GPU Type | Memory | Training Time | Cost/Hour | Total Cost |
|----------|--------|---------------|-----------|------------|
| **A100 40GB** | 40GB | 1-2 hours | ~$2.00 | $2.00-$4.00 |
| **A100 80GB** | 80GB | 45-90 min | ~$3.00 | $2.25-$4.50 |
| **V100** | 16GB | 2-3 hours | ~$1.00 | $2.00-$3.00 |
| **H100** | 80GB | 30-60 min | ~$4.00 | $2.00-$4.00 |

*Estimated for 50 epochs with 1,635 EGFR compounds*

## ðŸŽ¯ Multi-Target Training

Train all 6 targets in one session:
```bash
modal run modal_molbert.py --all-targets --webhook-url YOUR_WEBHOOK
```

**Targets trained:**
- EGFR, BRAF, CDK2, PARP1, BCL2, VEGFR2

**Total time:** ~6-12 hours for all targets
**Total cost:** ~$12-$24 (vs weeks on CPU)

## ðŸ”§ GPU Optimizations Included

- âœ… **Mixed Precision Training** (FP16) - 2x faster
- âœ… **Larger Batch Sizes** (64 vs 8 on CPU) 
- âœ… **Gradient Accumulation** for memory efficiency
- âœ… **Automatic GPU Memory Management**
- âœ… **Learning Rate Scheduling**
- âœ… **Early Stopping** to prevent overfitting

## ðŸ“± Progress Monitoring

Real-time updates sent to your webhook:
```json
{
  "status": "training",
  "message": "Epoch 25/50 completed", 
  "progress": 65.0,
  "target": "EGFR",
  "epoch": 25,
  "r2_score": 0.75,
  "loss": 0.234
}
```

## ðŸ’¾ Model Storage

- **During Training**: Checkpoints saved to Modal volume
- **After Training**: Download via Modal API or webhook
- **Persistent**: Models saved even after container stops

## ðŸš¨ Cost Management

- **Automatic Shutdown**: Container stops when training completes
- **Spot Instances**: Use Modal's preemptible GPUs for 50% savings
- **Early Stopping**: Prevents unnecessary training
- **Resource Monitoring**: Track GPU utilization

## ðŸ“¥ Downloading Results

```python
# Download trained model
import modal

app = modal.App.lookup("molbert-training")
download_func = app["download_trained_model"]

# Get model data
model_data = download_func.remote(target="EGFR")
with open("EGFR_molbert_trained.pkl", "wb") as f:
    f.write(model_data)
```

## ðŸ”„ Integration with Existing System

After GPU training completes, integrate back to your current system:

1. **Download Models**: Transfer trained models to `/app/backend/trained_molbert_models/`
2. **Update Predictor**: Use new models in existing `molbert_predictor.py`
3. **Performance Boost**: Expect significant RÂ² improvements (0.02 â†’ 0.6+)

## ðŸŽ¯ Expected Performance Improvements

| Metric | Current CPU (4 epochs) | Modal GPU (50 epochs) |
|--------|------------------------|----------------------|
| **RÂ² Score** | 0.018 | 0.60-0.85 |
| **Training Time** | 1-2 days | 1-2 hours |
| **Batch Size** | 8 | 64 |
| **Stability** | Limited | Production-ready |

## ðŸ“ž Support

- **Modal Docs**: https://modal.com/docs
- **GPU Pricing**: https://modal.com/pricing
- **Status Dashboard**: https://status.modal.com