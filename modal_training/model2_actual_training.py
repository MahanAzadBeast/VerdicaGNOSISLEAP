"""
Model 2 Actual Training - Cytotoxicity Prediction Model
Train on real GDSC + EPA InvitroDB data with ChemBERTa embeddings
"""

import modal
import pandas as pd
import numpy as np
import logging
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import pickle
from rdkit import Chem
from rdkit.Chem import Descriptors
from transformers import AutoTokenizer, AutoModel

# Modal setup with ML libraries
image = modal.Image.debian_slim(python_version="3.11").pip_install([
    "pandas", 
    "numpy",
    "torch",
    "torchvision", 
    "scikit-learn",
    "rdkit-pypi",
    "transformers",
    "wandb",
    "matplotlib",
    "seaborn"
])

app = modal.App("model2-actual-training")

# Persistent volume for datasets and models
datasets_volume = modal.Volume.from_name("expanded-datasets", create_if_missing=True)
models_volume = modal.Volume.from_name("cell-line-models", create_if_missing=True)

class ChemBERTaEncoder(nn.Module):
    """ChemBERTa molecular encoder for SMILES"""
    
    def __init__(self, model_name="seyonec/ChemBERTa-zinc-base-v1", embedding_dim=768):
        super().__init__()
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.chemberta = AutoModel.from_pretrained(model_name)
        self.projection = nn.Linear(embedding_dim, 256)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, smiles_list: List[str]) -> torch.Tensor:
        # Tokenize SMILES
        tokens = self.tokenizer(
            smiles_list, 
            padding=True, 
            truncation=True, 
            return_tensors="pt",
            max_length=512
        )
        
        # Move to same device as model
        device = next(self.parameters()).device
        tokens = {k: v.to(device) for k, v in tokens.items()}
        
        # Get ChemBERTa embeddings
        with torch.no_grad():
            outputs = self.chemberta(**tokens)
            pooled_output = outputs.pooler_output  # [batch_size, embedding_dim]
        
        # Project to desired dimension
        molecular_features = self.projection(pooled_output)
        molecular_features = self.dropout(molecular_features)
        
        return molecular_features

class GenomicEncoder(nn.Module):
    """Encoder for genomic features"""
    
    def __init__(self, input_dim: int = 100, hidden_dim: int = 256):
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 128)
        )
        
    def forward(self, genomic_features: torch.Tensor) -> torch.Tensor:
        return self.encoder(genomic_features)

class CytotoxicityPredictionModel(nn.Module):
    """Model 2: Cytotoxicity Prediction with Therapeutic Index"""
    
    def __init__(self, 
                 genomic_feature_dim: int = 100,
                 molecular_dim: int = 256,
                 genomic_dim: int = 128,
                 fusion_dim: int = 384):
        super().__init__()
        
        # Store dimensions as instance variables
        self.molecular_dim = molecular_dim
        self.genomic_dim = genomic_dim
        
        # Encoders
        self.molecular_encoder = ChemBERTaEncoder()
        self.genomic_encoder = GenomicEncoder(input_dim=genomic_feature_dim)
        
        # Cross-modal attention
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=molecular_dim,
            num_heads=8,
            batch_first=True
        )
        
        # Fusion layers
        self.fusion_layers = nn.Sequential(
            nn.Linear(molecular_dim + genomic_dim, fusion_dim),
            nn.ReLU(),
            nn.BatchNorm1d(fusion_dim),
            nn.Dropout(0.3),
            nn.Linear(fusion_dim, fusion_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Multi-task heads
        self.cancer_ic50_head = nn.Sequential(
            nn.Linear(fusion_dim // 2, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)  # Cancer IC50 prediction
        )
        
        self.normal_ac50_head = nn.Sequential(
            nn.Linear(fusion_dim // 2, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)  # Normal AC50 prediction
        )
        
        self.selectivity_head = nn.Sequential(
            nn.Linear(fusion_dim // 2, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)  # Direct selectivity index prediction
        )
        
        # Uncertainty estimation
        self.uncertainty_head = nn.Sequential(
            nn.Linear(fusion_dim // 2, 32),
            nn.ReLU(),
            nn.Linear(32, 3),  # Uncertainty for all 3 outputs
            nn.Softplus()
        )
        
    def forward(self, smiles_batch: List[str], genomic_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        
        # Encode molecular structure
        molecular_features = self.molecular_encoder(smiles_batch)
        
        # Encode genomic features
        genomic_features_encoded = self.genomic_encoder(genomic_features)
        
        # Cross-modal attention with dimension matching
        molecular_expanded = molecular_features.unsqueeze(1)  # [batch, 1, 256]
        
        # Project genomic features to match molecular dimension
        genomic_proj = nn.Linear(genomic_features_encoded.shape[-1], self.molecular_dim).to(genomic_features_encoded.device)
        genomic_matched = genomic_proj(genomic_features_encoded)  # [batch, 256]
        genomic_expanded = genomic_matched.unsqueeze(1)  # [batch, 1, 256]
        
        attended_molecular, _ = self.cross_attention(
            molecular_expanded, genomic_expanded, genomic_expanded
        )
        attended_molecular = attended_molecular.squeeze(1)  # [batch, 256]
        
        # Fusion
        fused_features = torch.cat([attended_molecular, genomic_features_encoded], dim=-1)
        fused_features = self.fusion_layers(fused_features)
        
        # Multi-task predictions
        cancer_ic50 = self.cancer_ic50_head(fused_features)
        normal_ac50 = self.normal_ac50_head(fused_features)
        selectivity_index = self.selectivity_head(fused_features)
        uncertainty = self.uncertainty_head(fused_features)
        
        return cancer_ic50, normal_ac50, selectivity_index, uncertainty

def extract_molecular_features(smiles: str) -> List[float]:
    """Extract RDKit molecular descriptors as backup features"""
    
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return [0.0] * 10
        
        features = [
            Descriptors.MolWt(mol),
            Descriptors.LogP(mol),
            Descriptors.NumHDonors(mol),
            Descriptors.NumHAcceptors(mol),
            Descriptors.TPSA(mol),
            Descriptors.NumRotatableBonds(mol),
            Descriptors.NumAromaticRings(mol),
            Descriptors.FractionCsp3(mol),
            Descriptors.BalabanJ(mol),
            Descriptors.BertzCT(mol)
        ]
        
        return [float(f) if not np.isnan(f) else 0.0 for f in features]
        
    except:
        return [0.0] * 10

@app.function(
    image=image,
    volumes={
        "/vol/datasets": datasets_volume,
        "/vol/models": models_volume
    },
    gpu="A10G",
    cpu=8.0,
    memory=32768,
    timeout=14400,  # 4 hours for training
    secrets=[modal.Secret.from_name("wandb-secret")]
)
def train_cytotoxicity_model():
    """
    Train Model 2: Cytotoxicity Prediction Model with Therapeutic Index
    Uses real GDSC + EPA InvitroDB data with ChemBERTa embeddings
    """
    
    logging.basicConfig(level=logging.INFO)
    print("üß¨ MODEL 2: CYTOTOXICITY PREDICTION TRAINING")
    print("=" * 80)
    print("üéØ Multi-task: Cancer IC50 + Normal AC50 + Selectivity Index")
    print("ü§ñ ChemBERTa molecular embeddings")
    print("üß¨ Genomic context for cell lines")
    print("‚úÖ Real GDSC + EPA InvitroDB data")
    
    # Initialize W&B
    import wandb
    wandb.init(
        project="gnosis-model2-cytotoxicity",
        name=f"model2-cytotox-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
        config={
            "model": "Cytotoxicity_Prediction_Model",
            "data": "GDSC_EPA_InvitroDB",
            "architecture": "ChemBERTa_Genomic_MultiTask",
            "tasks": ["Cancer_IC50", "Normal_AC50", "Selectivity_Index"]
        }
    )
    
    try:
        datasets_dir = Path("/vol/datasets")
        models_dir = Path("/vol/models")
        
        # Load Model 2 training data
        print("\nüìä STEP 1: Loading Model 2 training data...")
        
        training_data_path = datasets_dir / "gnosis_model2_cytotox_training.csv"
        
        if not training_data_path.exists():
            raise Exception("Model 2 training data not found. Run model2_training_starter.py first.")
        
        training_df = pd.read_csv(training_data_path)
        print(f"   ‚úÖ Training data loaded: {len(training_df):,} records")
        print(f"   üìä Columns: {list(training_df.columns)}")
        
        # Filter for records with valid SMILES and targets
        print("\nüîß STEP 2: Preparing training data...")
        
        # Remove records without SMILES
        training_df = training_df.dropna(subset=['SMILES'])
        training_df = training_df[training_df['SMILES'].str.len() >= 5]
        
        # Remove records without cancer IC50
        training_df = training_df.dropna(subset=['pic50_cancer'])
        
        print(f"   üìä After filtering: {len(training_df):,} records")
        print(f"   üìä Unique compounds: {training_df['SMILES'].nunique()}")
        
        # Prepare genomic features
        print("\nüß¨ STEP 3: Preparing genomic features...")
        
        genomic_cols = [col for col in training_df.columns if col.startswith('genomic_')]
        
        if not genomic_cols:
            # Create basic genomic features from available data
            genomic_features = pd.DataFrame()
            genomic_features['tissue_type_encoded'] = pd.get_dummies(training_df['tissue_type']).iloc[:, :10]
            
            # Add molecular features as genomic context
            for idx, smiles in enumerate(training_df['SMILES']):
                mol_features = extract_molecular_features(smiles)
                for i, feat in enumerate(mol_features[:10]):
                    genomic_features.at[idx, f'mol_feat_{i}'] = feat
            
            genomic_features = genomic_features.fillna(0)
            genomic_cols = genomic_features.columns.tolist()
        else:
            genomic_features = training_df[genomic_cols].fillna(0)
        
        print(f"   ‚úÖ Genomic features: {len(genomic_cols)} features")
        
        # Prepare targets (multi-task)
        print("\nüéØ STEP 4: Preparing multi-task targets...")
        
        # Cancer IC50 (always available)
        cancer_ic50_targets = training_df['pic50_cancer'].values
        
        # Normal AC50 (available for some compounds)
        normal_ac50_targets = training_df['pIC50'].fillna(-1).values  # -1 for missing
        has_normal_data = training_df['pIC50'].notna().values
        
        # Selectivity Index (available for some compounds) 
        selectivity_targets = training_df['selectivity_index'].fillna(0).values
        has_selectivity_data = training_df['selectivity_index'].notna().values
        
        print(f"   ‚úÖ Cancer IC50: {len(cancer_ic50_targets)} targets")
        print(f"   ‚úÖ Normal AC50: {has_normal_data.sum()} targets (of {len(normal_ac50_targets)})")
        print(f"   ‚úÖ Selectivity Index: {has_selectivity_data.sum()} targets")
        
        # Standardize features
        print("\n‚öñÔ∏è STEP 5: Standardizing features...")
        
        genomic_scaler = StandardScaler()
        genomic_features_scaled = genomic_scaler.fit_transform(genomic_features)
        
        # Train-test split
        print("\nüìä STEP 6: Creating train-test split...")
        
        indices = np.arange(len(training_df))
        train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
        
        print(f"   üìä Training samples: {len(train_idx):,}")
        print(f"   üìä Test samples: {len(test_idx):,}")
        
        # Convert to device tensors
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"   üíª Using device: {device}")
        
        # Prepare training batches
        train_smiles = training_df.iloc[train_idx]['SMILES'].tolist()
        train_genomics = torch.tensor(genomic_features_scaled[train_idx], dtype=torch.float32).to(device)
        train_cancer_ic50 = torch.tensor(cancer_ic50_targets[train_idx], dtype=torch.float32).to(device)
        train_normal_ac50 = torch.tensor(normal_ac50_targets[train_idx], dtype=torch.float32).to(device)
        train_selectivity = torch.tensor(selectivity_targets[train_idx], dtype=torch.float32).to(device)
        train_has_normal = torch.tensor(has_normal_data[train_idx], dtype=torch.bool).to(device)
        train_has_selectivity = torch.tensor(has_selectivity_data[train_idx], dtype=torch.bool).to(device)
        
        # Test data
        test_smiles = training_df.iloc[test_idx]['SMILES'].tolist()
        test_genomics = torch.tensor(genomic_features_scaled[test_idx], dtype=torch.float32).to(device)
        test_cancer_ic50 = torch.tensor(cancer_ic50_targets[test_idx], dtype=torch.float32).to(device)
        test_normal_ac50 = torch.tensor(normal_ac50_targets[test_idx], dtype=torch.float32).to(device)
        test_selectivity = torch.tensor(selectivity_targets[test_idx], dtype=torch.float32).to(device)
        test_has_normal = torch.tensor(has_normal_data[test_idx], dtype=torch.bool).to(device)
        test_has_selectivity = torch.tensor(has_selectivity_data[test_idx], dtype=torch.bool).to(device)
        
        # Initialize model
        print(f"\nü§ñ STEP 7: Initializing Cytotoxicity Prediction Model...")
        
        model = CytotoxicityPredictionModel(
            genomic_feature_dim=genomic_features_scaled.shape[1],
            molecular_dim=256,
            genomic_dim=128,
            fusion_dim=384
        ).to(device)
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"   ‚úÖ Model parameters: {total_params:,} total, {trainable_params:,} trainable")
        
        # Training setup
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=15, factor=0.5)
        
        # Training loop
        print(f"\nüèãÔ∏è STEP 8: Training Model 2...")
        
        model.train()
        batch_size = 16  # Smaller batch for ChemBERTa
        num_epochs = 200
        best_loss = float('inf')
        patience = 30
        no_improve_count = 0
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            # Shuffle training data
            perm = torch.randperm(len(train_idx))
            train_smiles_shuffled = [train_smiles[i] for i in perm]
            train_genomics_shuffled = train_genomics[perm]
            train_cancer_ic50_shuffled = train_cancer_ic50[perm]
            train_normal_ac50_shuffled = train_normal_ac50[perm]
            train_selectivity_shuffled = train_selectivity[perm]
            train_has_normal_shuffled = train_has_normal[perm]
            train_has_selectivity_shuffled = train_has_selectivity[perm]
            
            # Mini-batch training
            for i in range(0, len(train_idx), batch_size):
                end_idx = min(i + batch_size, len(train_idx))
                
                batch_smiles = train_smiles_shuffled[i:end_idx]
                batch_genomics = train_genomics_shuffled[i:end_idx]
                batch_cancer_ic50 = train_cancer_ic50_shuffled[i:end_idx]
                batch_normal_ac50 = train_normal_ac50_shuffled[i:end_idx]
                batch_selectivity = train_selectivity_shuffled[i:end_idx]
                batch_has_normal = train_has_normal_shuffled[i:end_idx]
                batch_has_selectivity = train_has_selectivity_shuffled[i:end_idx]
                
                optimizer.zero_grad()
                
                # Forward pass
                pred_cancer, pred_normal, pred_selectivity, uncertainty = model(batch_smiles, batch_genomics)
                
                pred_cancer = pred_cancer.squeeze()
                pred_normal = pred_normal.squeeze()
                pred_selectivity = pred_selectivity.squeeze()
                uncertainty = uncertainty.squeeze()
                
                # Multi-task loss with uncertainty weighting
                loss = 0.0
                
                # Cancer IC50 loss (always available)
                cancer_loss = F.mse_loss(pred_cancer, batch_cancer_ic50)
                loss += cancer_loss
                
                # Normal AC50 loss (only for compounds with normal data)
                if batch_has_normal.sum() > 0:
                    normal_mask = batch_has_normal
                    normal_loss = F.mse_loss(pred_normal[normal_mask], batch_normal_ac50[normal_mask])
                    loss += 0.5 * normal_loss  # Lower weight
                
                # Selectivity Index loss (only for compounds with selectivity data)
                if batch_has_selectivity.sum() > 0:
                    selectivity_mask = batch_has_selectivity
                    selectivity_loss = F.mse_loss(pred_selectivity[selectivity_mask], batch_selectivity[selectivity_mask])
                    loss += 0.3 * selectivity_loss  # Lower weight
                
                # Uncertainty regularization
                uncertainty_reg = torch.mean(uncertainty)
                loss += 0.001 * uncertainty_reg
                
                # Backward pass
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches
            
            # Validation every 5 epochs
            if epoch % 5 == 0:
                model.eval()
                with torch.no_grad():
                    # Evaluate on test set in batches
                    test_losses = []
                    test_cancer_r2s = []
                    
                    for i in range(0, len(test_idx), batch_size):
                        end_idx = min(i + batch_size, len(test_idx))
                        
                        batch_test_smiles = test_smiles[i:end_idx]
                        batch_test_genomics = test_genomics[i:end_idx]
                        batch_test_cancer = test_cancer_ic50[i:end_idx]
                        
                        pred_test_cancer, pred_test_normal, pred_test_selectivity, test_uncertainty = model(
                            batch_test_smiles, batch_test_genomics
                        )
                        
                        pred_test_cancer = pred_test_cancer.squeeze()
                        test_loss = F.mse_loss(pred_test_cancer, batch_test_cancer)
                        test_losses.append(test_loss.item())
                        
                        # R¬≤ for cancer IC50
                        cancer_r2 = r2_score(
                            batch_test_cancer.cpu().numpy(), 
                            pred_test_cancer.cpu().numpy()
                        )
                        test_cancer_r2s.append(cancer_r2)
                    
                    avg_test_loss = np.mean(test_losses)
                    avg_cancer_r2 = np.mean(test_cancer_r2s)
                    
                    print(f"   Epoch {epoch:3d}: Loss={avg_loss:.4f} | Test Loss={avg_test_loss:.4f} | Cancer R¬≤={avg_cancer_r2:.4f}")
                    
                    # W&B logging
                    wandb.log({
                        "epoch": epoch,
                        "train_loss": avg_loss,
                        "test_loss": avg_test_loss,
                        "cancer_ic50_r2": avg_cancer_r2
                    })
                    
                    if avg_test_loss < best_loss:
                        best_loss = avg_test_loss
                        best_model_state = model.state_dict().copy()
                        no_improve_count = 0
                    else:
                        no_improve_count += 1
                
                model.train()
                scheduler.step(avg_loss)
                
                # Early stopping
                if no_improve_count >= patience:
                    print(f"   Early stopping at epoch {epoch}")
                    break
        
        # Save best model
        print(f"\nüíæ STEP 9: Saving Model 2...")
        
        models_dir.mkdir(parents=True, exist_ok=True)
        
        model_save_path = models_dir / "model2_cytotoxicity_prediction.pth"
        torch.save({
            'model_state_dict': best_model_state,
            'genomic_scaler': genomic_scaler,
            'genomic_feature_columns': genomic_cols,
            'best_test_loss': best_loss,
            'model_config': {
                'genomic_feature_dim': genomic_features_scaled.shape[1],
                'molecular_dim': 256,
                'genomic_dim': 128,
                'fusion_dim': 384
            },
            'training_statistics': {
                'total_samples': len(training_df),
                'unique_compounds': int(training_df['SMILES'].nunique()),
                'with_selectivity_data': int(has_selectivity_data.sum()),
                'with_normal_data': int(has_normal_data.sum())
            }
        }, model_save_path)
        
        # Create metadata
        metadata = {
            'model_type': 'Model2_Cytotoxicity_Prediction',
            'architecture': 'ChemBERTa_Genomic_MultiTask',
            'tasks': ['Cancer_IC50', 'Normal_AC50', 'Selectivity_Index'],
            'data_sources': ['GDSC_real', 'EPA_InvitroDB_v4.1'],
            'training_samples': len(train_idx),
            'test_samples': len(test_idx),
            'best_test_loss': float(best_loss),
            'unique_compounds': int(training_df['SMILES'].nunique()),
            'genomic_features': len(genomic_cols),
            'with_selectivity_index': int(has_selectivity_data.sum()),
            'with_normal_toxicity': int(has_normal_data.sum()),
            'molecular_encoder': 'ChemBERTa_zinc_base_v1',
            'real_experimental_data': True,
            'no_synthetic_data': True,
            'training_timestamp': datetime.now().isoformat(),
            'ready_for_inference': True
        }
        
        metadata_path = models_dir / "model2_cytotoxicity_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # Final evaluation
        print(f"\nüéâ MODEL 2 TRAINING COMPLETED!")
        print("=" * 80)
        print(f"üìÅ Model: {model_save_path}")
        print(f"üìÅ Metadata: {metadata_path}")
        
        print(f"\nüìä Training Results:")
        print(f"  ‚Ä¢ Training samples: {len(train_idx):,}")
        print(f"  ‚Ä¢ Test samples: {len(test_idx):,}")
        print(f"  ‚Ä¢ Best test loss: {best_loss:.4f}")
        print(f"  ‚Ä¢ Unique compounds: {training_df['SMILES'].nunique()}")
        print(f"  ‚Ä¢ With selectivity data: {has_selectivity_data.sum():,}")
        print(f"  ‚Ä¢ With normal toxicity: {has_normal_data.sum():,}")
        
        print(f"\n‚úÖ MODEL 2 FEATURES:")
        print(f"  ‚Ä¢ Multi-task: Cancer IC50 + Normal AC50 + Selectivity Index")
        print(f"  ‚Ä¢ ChemBERTa molecular embeddings")
        print(f"  ‚Ä¢ Genomic context integration")  
        print(f"  ‚Ä¢ Real GDSC + EPA InvitroDB data")
        print(f"  ‚Ä¢ Therapeutic index calculation")
        print(f"  ‚Ä¢ Uncertainty quantification")
        
        wandb.finish()
        
        return {
            'status': 'success',
            'model_type': 'Model2_Cytotoxicity_Prediction',
            'training_samples': len(train_idx),
            'test_samples': len(test_idx),
            'best_test_loss': float(best_loss),
            'unique_compounds': int(training_df['SMILES'].nunique()),
            'with_selectivity_data': int(has_selectivity_data.sum()),
            'with_normal_data': int(has_normal_data.sum()),
            'genomic_features': len(genomic_cols),
            'real_experimental_data': True,
            'model_path': str(model_save_path),
            'metadata_path': str(metadata_path),
            'ready_for_inference': True,
            'training_completed': True
        }
        
    except Exception as e:
        print(f"‚ùå MODEL 2 TRAINING FAILED: {e}")
        import traceback
        traceback.print_exc()
        wandb.finish()
        
        return {
            'status': 'failed',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

if __name__ == "__main__":
    print("üß¨ Model 2: Cytotoxicity Prediction Training")