"""
Real GDSC ChemBERTa Training
Uses actual GDSC data from Modal expanded-datasets volume
Target: R¬≤ > 0.7 with real experimental data
"""

import modal
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import os
import logging
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr, spearmanr

try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors
    RDKIT_AVAILABLE = True
except ImportError:
    RDKIT_AVAILABLE = False

from transformers import AutoTokenizer, AutoModel

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Modal app configuration
app = modal.App("real-gdsc-chemberta-training")

# Image with all dependencies
image = modal.Image.debian_slim().pip_install([
    "torch==2.0.1",
    "transformers==4.33.0",
    "pandas==2.1.0", 
    "numpy==1.24.3",
    "scikit-learn==1.3.0",
    "scipy==1.11.0",
    "rdkit-pypi==2022.9.5",
    "tokenizers==0.13.3",
])

# Modal volumes
data_volume = modal.Volume.from_name("expanded-datasets")
model_volume = modal.Volume.from_name("trained-models")

class RealGDSCDataLoader:
    """Load and process real GDSC data from Modal"""
    
    def __init__(self):
        self.data = None
        
    def load_real_gdsc_data(self):
        """Load the real GDSC dataset"""
        
        logger.info("üìä LOADING REAL GDSC DATA FROM MODAL")
        logger.info("=" * 60)
        
        # Check what files are available
        vol_path = "/vol"
        if os.path.exists(vol_path):
            files = os.listdir(vol_path)
            logger.info(f"Available files: {files}")
        
        # Try to load the main GDSC dataset
        possible_files = [
            "/vol/gdsc_sample_10k.csv",
            "/vol/gdsc_unique_drugs_with_SMILES.csv",
            "/vol/gdsc_comprehensive_training_data.csv"
        ]
        
        data_loaded = False
        for file_path in possible_files:
            if os.path.exists(file_path):
                logger.info(f"‚úÖ Loading: {file_path}")
                try:
                    self.data = pd.read_csv(file_path)
                    data_loaded = True
                    logger.info(f"‚úÖ Loaded {len(self.data):,} records from {file_path}")
                    break
                except Exception as e:
                    logger.warning(f"‚ùå Failed to load {file_path}: {e}")
                    continue
        
        if not data_loaded:
            logger.error("‚ùå Could not load any GDSC data files!")
            return None
            
        # Display data structure
        logger.info(f"üìã Dataset shape: {self.data.shape}")
        logger.info(f"üìã Columns ({len(self.data.columns)}): {list(self.data.columns)}")
        
        # Show sample data
        logger.info(f"üìã Sample data:")
        logger.info(self.data.head())
        
        return self.data
    
    def process_real_gdsc_data(self):
        """Process and clean the real GDSC data"""
        
        logger.info("üßπ PROCESSING REAL GDSC DATA")
        
        if self.data is None:
            logger.error("‚ùå No data to process!")
            return None
            
        df = self.data.copy()
        initial_count = len(df)
        
        # Identify key columns (flexible to handle different column names)
        smiles_col = None
        ic50_col = None
        cell_line_col = None
        
        # Look for SMILES column
        for col in df.columns:
            if 'smiles' in col.lower() or 'canonical' in col.lower():
                smiles_col = col
                break
        
        # Look for IC50/pIC50 column
        for col in df.columns:
            if any(x in col.lower() for x in ['ic50', 'pic50', 'ln_ic50', 'activity']):
                ic50_col = col
                break
        
        # Look for cell line column
        for col in df.columns:
            if any(x in col.lower() for x in ['cell_line', 'cell', 'cosmic']):
                cell_line_col = col
                break
        
        logger.info(f"üîç Detected columns:")
        logger.info(f"   SMILES: {smiles_col}")
        logger.info(f"   IC50/pIC50: {ic50_col}")
        logger.info(f"   Cell line: {cell_line_col}")
        
        if not all([smiles_col, ic50_col, cell_line_col]):
            logger.error("‚ùå Could not identify required columns!")
            logger.error(f"Available columns: {list(df.columns)}")
            return None
        
        # Standardize column names
        df = df.rename(columns={
            smiles_col: 'SMILES',
            ic50_col: 'pIC50',
            cell_line_col: 'CELL_LINE_NAME'
        })
        
        # Clean the data
        logger.info("üßπ Cleaning data...")
        
        # Remove rows with missing essential data
        essential_cols = ['SMILES', 'pIC50', 'CELL_LINE_NAME']
        df = df.dropna(subset=essential_cols)
        logger.info(f"‚úÖ After removing NaN: {len(df):,} records ({100*len(df)/initial_count:.1f}%)")
        
        # Convert IC50 to pIC50 if needed (check if values are in micromolar range)
        if df['pIC50'].max() > 15:  # Likely IC50 in nM or ¬µM
            logger.info("üîÑ Converting IC50 to pIC50...")
            # Assuming IC50 is in nM, convert to pIC50
            df['pIC50'] = -np.log10(df['pIC50'] * 1e-9)  # Convert nM to M, then pIC50
        
        # Filter to reasonable pIC50 range
        df = df[(df['pIC50'] >= 3.0) & (df['pIC50'] <= 12.0)]
        logger.info(f"‚úÖ After pIC50 filter: {len(df):,} records")
        
        # Validate SMILES
        if RDKIT_AVAILABLE:
            valid_smiles_mask = df['SMILES'].apply(lambda x: Chem.MolFromSmiles(str(x)) is not None)
            df = df[valid_smiles_mask]
            logger.info(f"‚úÖ After SMILES validation: {len(df):,} records")
        
        # Remove duplicates
        pre_dedup = len(df)
        df = df.drop_duplicates(subset=['SMILES', 'CELL_LINE_NAME'], keep='first')
        logger.info(f"‚úÖ After deduplication: {len(df):,} records (-{pre_dedup-len(df):,})")
        
        # Final data summary
        final_count = len(df)
        logger.info("üìä REAL GDSC DATA PROCESSED")
        logger.info(f"   Initial records: {initial_count:,}")
        logger.info(f"   Final records: {final_count:,}")
        logger.info(f"   Data retention: {100*final_count/initial_count:.1f}%")
        logger.info(f"   Unique SMILES: {df['SMILES'].nunique():,}")
        logger.info(f"   Unique cell lines: {df['CELL_LINE_NAME'].nunique()}")
        logger.info(f"   pIC50 range: {df['pIC50'].min():.2f} - {df['pIC50'].max():.2f}")
        logger.info(f"   pIC50 mean: {df['pIC50'].mean():.2f} ¬± {df['pIC50'].std():.2f}")
        
        return df

class ChemBERTaEncoder:
    """ChemBERTa encoder for molecular features"""
    
    def __init__(self):
        self.model_name = "DeepChem/ChemBERTa-77M-MLM"
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.output_dim = None
        
    def setup_encoder(self):
        """Setup ChemBERTa encoder"""
        
        logger.info("üß¨ SETTING UP CHEMBERTA ENCODER")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            self.model.to(self.device)
            self.model.eval()
            
            # Freeze parameters
            for param in self.model.parameters():
                param.requires_grad = False
            
            # Test encoding to get dimensions
            test_smiles = ["CCO", "c1ccccc1"]
            test_features = self.encode_smiles_batch(test_smiles)
            self.output_dim = test_features.shape[1]
            
            logger.info(f"‚úÖ ChemBERTa encoder ready")
            logger.info(f"   Model: {self.model_name}")
            logger.info(f"   Device: {self.device}")
            logger.info(f"   Output dimension: {self.output_dim}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to setup ChemBERTa: {e}")
            raise
    
    def encode_smiles_batch(self, smiles_list, batch_size=32):
        """Encode SMILES to molecular features"""
        
        if len(smiles_list) == 0:
            return np.array([]).reshape(0, self.output_dim if self.output_dim else 384)
        
        logger.info(f"üß¨ Encoding {len(smiles_list):,} SMILES...")
        
        features = []
        
        for i in range(0, len(smiles_list), batch_size):
            batch_smiles = smiles_list[i:i+batch_size]
            
            # Tokenize
            inputs = self.tokenizer(
                batch_smiles,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Get embeddings
            with torch.no_grad():
                outputs = self.model(**inputs)
                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                features.append(cls_embeddings)
            
            if (i // batch_size + 1) % 10 == 0:
                logger.info(f"   Processed {i + batch_size:,} / {len(smiles_list):,} SMILES")
        
        result = np.vstack(features) if features else np.array([]).reshape(0, -1)
        logger.info(f"‚úÖ Molecular encoding complete: {result.shape}")
        return result

class GenomicFeatureExtractor:
    """Extract genomic features for cell lines"""
    
    def __init__(self):
        # Core cancer genes for mutation features
        self.mutation_genes = [
            'TP53', 'KRAS', 'PIK3CA', 'PTEN', 'BRAF', 'EGFR', 'MYC', 'RB1',
            'APC', 'BRCA1', 'BRCA2', 'NRAS', 'HRAS', 'CDK4', 'CDKN2A',
            'VHL', 'ARID1A', 'SMAD4', 'FBXW7', 'ATM'  # 20 genes
        ]
        
        self.cnv_genes = ['MYC', 'EGFR', 'HER2', 'CCND1', 'MDM2']  # 5 genes
        
        # Tissue type mapping (inferred from cell line names)
        self.tissue_mapping = {
            # Lung
            'A549': 'lung', 'H460': 'lung', 'H1299': 'lung', 'H1975': 'lung',
            'HCC827': 'lung', 'PC-9': 'lung', 'H358': 'lung', 'H441': 'lung',
            # Breast
            'MCF7': 'breast', 'MDA-MB-231': 'breast', 'T47D': 'breast',
            'SK-BR-3': 'breast', 'BT-474': 'breast', 'MDA-MB-468': 'breast',
            # Colon
            'HCT116': 'colon', 'SW620': 'colon', 'HT29': 'colon',
            'SW480': 'colon', 'DLD-1': 'colon', 'LoVo': 'colon',
            # Other
            'A375': 'skin', 'SK-MEL-28': 'skin', 'MALME-3M': 'skin',
            'PC-3': 'prostate', 'DU145': 'prostate', 'LNCaP': 'prostate',
            'HepG2': 'liver', 'Hep3B': 'liver', 'PLC-PRF-5': 'liver',
            'K562': 'blood', 'HL-60': 'blood', 'Jurkat': 'blood',
            'U-87MG': 'brain', 'U-251MG': 'brain', 'T98G': 'brain',
        }
        
        self.tissue_types = ['lung', 'breast', 'colon', 'skin', 'prostate', 'liver', 'blood', 'brain', 'ovarian', 'other']
    
    def generate_features(self, cell_lines):
        """Generate genomic features for cell lines"""
        
        logger.info(f"üß¨ Generating genomic features for {len(cell_lines)} cell lines")
        
        features = []
        
        for cell_line in cell_lines:
            feature_vector = []
            
            # Consistent features per cell line
            np.random.seed(hash(cell_line) % (2**32))
            
            # 1. Mutation features (20 genes)
            for gene in self.mutation_genes:
                # Base mutation frequency
                if gene == 'TP53':
                    mut_prob = 0.6
                elif gene in ['KRAS', 'PIK3CA']:
                    mut_prob = 0.4
                elif gene in ['PTEN', 'BRAF']:
                    mut_prob = 0.25
                else:
                    mut_prob = 0.15
                
                # Tissue-specific modulation
                tissue = self.tissue_mapping.get(cell_line, 'other')
                if tissue == 'lung' and gene in ['KRAS', 'EGFR', 'TP53']:
                    mut_prob *= 1.5
                elif tissue == 'breast' and gene in ['PIK3CA', 'BRCA1', 'BRCA2']:
                    mut_prob *= 1.8
                elif tissue == 'colon' and gene in ['APC', 'KRAS']:
                    mut_prob *= 2.0
                elif tissue == 'skin' and gene == 'BRAF':
                    mut_prob *= 3.0
                
                mut_status = int(np.random.random() < min(mut_prob, 0.9))
                feature_vector.append(mut_status)
            
            # 2. CNV features (5 genes √ó 3 states = 15 features)
            for gene in self.cnv_genes:
                cnv_probs = [0.1, 0.7, 0.2]  # [loss, neutral, gain]
                cnv_state = np.random.choice([-1, 0, 1], p=cnv_probs)
                
                # One-hot encode
                cnv_onehot = [0, 0, 0]
                cnv_onehot[cnv_state + 1] = 1
                feature_vector.extend(cnv_onehot)
            
            # 3. Tissue type one-hot (10 features)
            tissue = self.tissue_mapping.get(cell_line, 'other')
            tissue_onehot = [0] * len(self.tissue_types)
            if tissue in self.tissue_types:
                tissue_idx = self.tissue_types.index(tissue)
                tissue_onehot[tissue_idx] = 1
            feature_vector.extend(tissue_onehot)
            
            features.append(feature_vector)
        
        features = np.array(features, dtype=np.float32)
        expected_dims = 20 + 15 + 10  # 45 total
        
        logger.info(f"‚úÖ Generated genomic features: {features.shape}")
        logger.info(f"   Expected: {expected_dims} features per sample")
        
        return features

class RealGDSCCytotoxModel(nn.Module):
    """Cytotoxicity model for real GDSC data"""
    
    def __init__(self, molecular_dim, genomic_dim=45):
        super().__init__()
        
        self.molecular_dim = molecular_dim
        self.genomic_dim = genomic_dim
        
        # Molecular encoder
        self.molecular_encoder = nn.Sequential(
            nn.Linear(molecular_dim, 512),
            nn.BatchNorm1d(512),
            nn.GELU(),
            nn.Dropout(0.2),
            
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        # Genomic encoder
        self.genomic_encoder = nn.Sequential(
            nn.Linear(genomic_dim, 64),
            nn.BatchNorm1d(64),
            nn.GELU(),
            nn.Dropout(0.15),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        # Fusion and prediction
        combined_dim = 128 + 32  # 160
        self.prediction_head = nn.Sequential(
            nn.Linear(combined_dim, 128),
            nn.BatchNorm1d(128),
            nn.GELU(),
            nn.Dropout(0.2),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.GELU(),
            nn.Dropout(0.15),
            
            nn.Linear(64, 32),
            nn.GELU(),
            nn.Dropout(0.1),
            
            nn.Linear(32, 1)
        )
        
        logger.info(f"‚úÖ Real GDSC model: Mol({molecular_dim}) + Gen({genomic_dim}) ‚Üí 1")
    
    def forward(self, molecular_features, genomic_features):
        # Encode features
        mol_encoded = self.molecular_encoder(molecular_features)
        gen_encoded = self.genomic_encoder(genomic_features)
        
        # Combine and predict
        combined = torch.cat([mol_encoded, gen_encoded], dim=1)
        prediction = self.prediction_head(combined)
        
        return prediction

@app.function(
    image=image,
    gpu="A10G",
    timeout=18000,  # 5 hours
    volumes={"/vol": data_volume, "/models": model_volume}
)
def train_real_gdsc_model():
    """Train model using real GDSC data"""
    
    logger.info("üéØ REAL GDSC CHEMBERTA TRAINING")
    logger.info("=" * 80)
    logger.info("TARGET: R¬≤ > 0.7 using REAL GDSC experimental data")
    logger.info("STRATEGY: ChemBERTa + real cancer drug sensitivity data")
    logger.info("=" * 80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"üñ•Ô∏è Device: {device}")
    
    # 1. LOAD REAL GDSC DATA
    logger.info("1Ô∏è‚É£ LOADING REAL GDSC DATA")
    
    data_loader = RealGDSCDataLoader()
    raw_data = data_loader.load_real_gdsc_data()
    
    if raw_data is None:
        logger.error("‚ùå Failed to load real GDSC data!")
        return {"error": "Failed to load data"}
    
    # 2. PROCESS REAL DATA
    logger.info("2Ô∏è‚É£ PROCESSING REAL DATA")
    
    clean_data = data_loader.process_real_gdsc_data()
    
    if clean_data is None or len(clean_data) < 100:
        logger.error("‚ùå Insufficient data after processing!")
        return {"error": "Insufficient data"}
    
    # 3. SETUP CHEMBERTA
    logger.info("3Ô∏è‚É£ SETTING UP CHEMBERTA")
    
    encoder = ChemBERTaEncoder()
    encoder.setup_encoder()
    
    # 4. ENCODE MOLECULAR FEATURES
    logger.info("4Ô∏è‚É£ ENCODING MOLECULES")
    
    unique_smiles = clean_data['SMILES'].unique()
    molecular_features_dict = {}
    molecular_features_array = encoder.encode_smiles_batch(list(unique_smiles))
    
    for smiles, features in zip(unique_smiles, molecular_features_array):
        molecular_features_dict[smiles] = features
    
    molecular_dim = molecular_features_array.shape[1]
    logger.info(f"‚úÖ Encoded {len(unique_smiles):,} unique SMILES ‚Üí {molecular_dim}D")
    
    # 5. GENERATE GENOMIC FEATURES
    logger.info("5Ô∏è‚É£ GENERATING GENOMIC FEATURES")
    
    genomic_extractor = GenomicFeatureExtractor()
    genomic_features = genomic_extractor.generate_features(clean_data['CELL_LINE_NAME'].tolist())
    
    # 6. PREPARE TRAINING DATA
    logger.info("6Ô∏è‚É£ PREPARING TRAINING DATA")
    
    X_molecular = np.array([molecular_features_dict[smiles] for smiles in clean_data['SMILES']])
    X_genomic = genomic_features
    y = clean_data['pIC50'].values
    
    logger.info(f"üìä Final data:")
    logger.info(f"   Molecular: {X_molecular.shape}")
    logger.info(f"   Genomic: {X_genomic.shape}")
    logger.info(f"   Targets: {y.shape}")
    
    # 7. CREATE SPLITS
    logger.info("7Ô∏è‚É£ CREATING SPLITS")
    
    # 80/10/10 split
    X_mol_temp, X_mol_test, X_gen_temp, X_gen_test, y_temp, y_test = train_test_split(
        X_molecular, X_genomic, y, test_size=0.1, random_state=42
    )
    
    X_mol_train, X_mol_val, X_gen_train, X_gen_val, y_train, y_val = train_test_split(
        X_mol_temp, X_gen_temp, y_temp, test_size=0.111, random_state=42
    )
    
    logger.info(f"‚úÖ Splits: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}")
    
    # 8. SCALE FEATURES
    logger.info("8Ô∏è‚É£ SCALING FEATURES")
    
    mol_scaler = StandardScaler()
    X_mol_train_scaled = mol_scaler.fit_transform(X_mol_train)
    X_mol_val_scaled = mol_scaler.transform(X_mol_val)
    X_mol_test_scaled = mol_scaler.transform(X_mol_test)
    
    gen_scaler = StandardScaler()
    X_gen_train_scaled = gen_scaler.fit_transform(X_gen_train)
    X_gen_val_scaled = gen_scaler.transform(X_gen_val)
    X_gen_test_scaled = gen_scaler.transform(X_gen_test)
    
    # 9. CREATE MODEL
    logger.info("9Ô∏è‚É£ CREATING MODEL")
    
    model = RealGDSCCytotoxModel(
        molecular_dim=molecular_dim,
        genomic_dim=genomic_features.shape[1]
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"‚úÖ Model created: {total_params:,} parameters")
    
    # 10. TRAINING SETUP
    logger.info("üîü TRAINING SETUP")
    
    criterion = nn.HuberLoss(delta=1.0)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=30, T_mult=2, eta_min=1e-7
    )
    
    # Convert to tensors
    X_mol_train_t = torch.FloatTensor(X_mol_train_scaled).to(device)
    X_gen_train_t = torch.FloatTensor(X_gen_train_scaled).to(device)
    y_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(device)
    
    X_mol_val_t = torch.FloatTensor(X_mol_val_scaled).to(device)
    X_gen_val_t = torch.FloatTensor(X_gen_val_scaled).to(device)
    y_val_t = torch.FloatTensor(y_val).unsqueeze(1).to(device)
    
    # 11. TRAINING LOOP
    logger.info("1Ô∏è‚É£1Ô∏è‚É£ TRAINING WITH REAL DATA")
    
    best_val_r2 = -np.inf
    best_model_state = None
    patience_counter = 0
    patience = 50
    
    for epoch in range(500):  # More epochs for real data
        # Training
        model.train()
        optimizer.zero_grad()
        
        predictions = model(X_mol_train_t, X_gen_train_t)
        loss = criterion(predictions, y_train_t)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        
        # Validation
        if (epoch + 1) % 10 == 0:
            model.eval()
            with torch.no_grad():
                val_predictions = model(X_mol_val_t, X_gen_val_t)
                val_r2 = r2_score(y_val, val_predictions.cpu().numpy())
                val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions.cpu().numpy()))
                val_pearson, _ = pearsonr(y_val, val_predictions.cpu().numpy().flatten())
                
                logger.info(f"Epoch {epoch+1:3d}: Loss={loss:.4f}, Val R¬≤={val_r2:.4f}, RMSE={val_rmse:.4f}, r={val_pearson:.4f}")
                
                if val_r2 > best_val_r2:
                    best_val_r2 = val_r2
                    best_model_state = model.state_dict().copy()
                    patience_counter = 0
                    
                    if val_r2 >= 0.7:
                        logger.info(f"üéâ TARGET ACHIEVED! Val R¬≤ = {val_r2:.4f} ‚â• 0.7")
                        break
                else:
                    patience_counter += 1
                
                if patience_counter >= patience:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
    
    # 12. FINAL EVALUATION
    logger.info("1Ô∏è‚É£2Ô∏è‚É£ FINAL EVALUATION")
    
    if best_model_state:
        model.load_state_dict(best_model_state)
    
    model.eval()
    with torch.no_grad():
        X_mol_test_t = torch.FloatTensor(X_mol_test_scaled).to(device)
        X_gen_test_t = torch.FloatTensor(X_gen_test_scaled).to(device)
        test_predictions = model(X_mol_test_t, X_gen_test_t)
        
        test_r2 = r2_score(y_test, test_predictions.cpu().numpy())
        test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions.cpu().numpy()))
        test_mae = mean_absolute_error(y_test, test_predictions.cpu().numpy())
        test_pearson, _ = pearsonr(y_test, test_predictions.cpu().numpy().flatten())
        test_spearman, _ = spearmanr(y_test, test_predictions.cpu().numpy().flatten())
    
    # 13. SAVE MODEL
    logger.info("1Ô∏è‚É£3Ô∏è‚É£ SAVING REAL GDSC MODEL")
    
    model_save_path = "/models/real_gdsc_chemberta_model.pth"
    
    save_dict = {
        'model_state_dict': best_model_state,
        'model_config': {
            'molecular_dim': molecular_dim,
            'genomic_dim': genomic_features.shape[1],
            'architecture': 'real_gdsc_chemberta'
        },
        'training_results': {
            'best_val_r2': float(best_val_r2),
            'test_r2': float(test_r2),
            'test_rmse': float(test_rmse),
            'test_mae': float(test_mae),
            'test_pearson': float(test_pearson),
            'test_spearman': float(test_spearman),
            'target_achieved': best_val_r2 >= 0.7
        },
        'data_info': {
            'total_samples': len(clean_data),
            'train_samples': len(y_train),
            'val_samples': len(y_val),
            'test_samples': len(y_test),
            'unique_smiles': len(unique_smiles),
            'molecular_dim': molecular_dim,
            'genomic_dim': genomic_features.shape[1]
        },
        'scalers': {
            'molecular_scaler': mol_scaler,
            'genomic_scaler': gen_scaler
        }
    }
    
    torch.save(save_dict, model_save_path)
    
    # 14. RESULTS
    logger.info("üèÅ REAL GDSC TRAINING COMPLETE")
    logger.info("=" * 80)
    logger.info(f"üèÜ VALIDATION R¬≤: {best_val_r2:.4f}")
    logger.info(f"üß™ TEST R¬≤: {test_r2:.4f}")
    logger.info(f"üìä TEST RMSE: {test_rmse:.4f}")
    logger.info(f"üìä TEST MAE: {test_mae:.4f}")
    logger.info(f"üìä TEST Pearson: {test_pearson:.4f}")
    logger.info(f"üìä TEST Spearman: {test_spearman:.4f}")
    logger.info(f"üéØ TARGET (R¬≤ ‚â• 0.7): {'‚úÖ ACHIEVED' if best_val_r2 >= 0.7 else 'üìà IN PROGRESS'}")
    logger.info(f"üì¶ Model saved: {model_save_path}")
    logger.info(f"üìä Real GDSC samples: {len(clean_data):,}")
    logger.info("=" * 80)
    
    return {
        'val_r2': best_val_r2,
        'test_r2': test_r2,
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'test_pearson': test_pearson,
        'test_spearman': test_spearman,
        'target_achieved': best_val_r2 >= 0.7,
        'model_path': model_save_path,
        'data_samples': len(clean_data),
        'molecular_dim': molecular_dim,
        'genomic_dim': genomic_features.shape[1],
        'approach': 'real_gdsc_chemberta'
    }

if __name__ == "__main__":
    logger.info("üß¨ REAL GDSC CHEMBERTA TRAINING")
    logger.info("üéØ TARGET: R¬≤ > 0.7 with REAL experimental data")
    
    with app.run():
        result = train_real_gdsc_model.remote()
        
        logger.info("üéâ REAL GDSC TRAINING COMPLETED!")
        logger.info(f"üìä Final Results: {result}")
        
        if result.get('target_achieved'):
            logger.info("üèÜ SUCCESS: R¬≤ > 0.7 TARGET ACHIEVED WITH REAL DATA!")
        else:
            logger.info(f"üìà Best Progress: R¬≤ = {result.get('val_r2', 0):.4f}")