"""
Local ChemBERTa Cytotoxicity Training (Without Modal)
Generate realistic GDSC-like data locally and train ChemBERTa + cytotox head
Target: R¬≤ > 0.7
"""

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import os
from pathlib import Path
import json
import logging
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Scientific computing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr, spearmanr
import pickle

# Molecular processing
try:
    from rdkit import Chem
    from rdkit.Chem import Scaffolds, Descriptors
    from rdkit.Chem.Scaffolds import MurckoScaffold
    RDKIT_AVAILABLE = True
except ImportError:
    RDKIT_AVAILABLE = False
    logging.warning("RDKit not available - will use simplified molecules")

# Transformer model
from transformers import AutoTokenizer, AutoModel

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SyntheticGDSCDataGenerator:
    """Generate realistic synthetic GDSC-like data for training"""
    
    def __init__(self):
        # Real GDSC cell lines (major cancer types)
        self.cell_lines = {
            # Lung cancer
            'A549': 'lung', 'H460': 'lung', 'H1299': 'lung', 'H1975': 'lung', 'H441': 'lung',
            # Breast cancer  
            'MCF7': 'breast', 'MDA-MB-231': 'breast', 'T47D': 'breast', 'BT-474': 'breast',
            # Colon cancer
            'HCT116': 'colon', 'SW620': 'colon', 'COLO320DM': 'colon', 'HT29': 'colon',
            # Skin cancer
            'A375': 'skin', 'SK-MEL-28': 'skin', 'MALME-3M': 'skin',
            # Prostate cancer
            'PC-3': 'prostate', 'DU145': 'prostate', 'LNCaP': 'prostate',
            # Liver cancer
            'HepG2': 'liver', 'Hep3B': 'liver', 'PLC-PRF-5': 'liver',
            # Blood cancer
            'K562': 'blood', 'HL-60': 'blood', 'Jurkat': 'blood', 'Raji': 'blood',
            # Brain cancer
            'U-87MG': 'brain', 'U-251MG': 'brain', 'T98G': 'brain',
            # Ovarian cancer
            'OVCAR-8': 'ovarian', 'OVCAR-3': 'ovarian', 'SK-OV-3': 'ovarian'
        }
        
        # Diverse drug SMILES from different chemical classes
        self.drug_smiles = [
            # Kinase inhibitors
            'CC1=C(C=C(C=C1)NC2=NC=CC(=N2)C3=CN=CC=C3)C(=O)C',  # Imatinib-like
            'C1CCC(CC1)NC2=NC=C(C(=N2)N)C3=CC=CC=C3F',          # Kinase inhibitor
            'CC(C)C1=NC=C(N1)C2=CC=C(C=C2)NC3=CC=C(C=C3)C',     # Small kinase inhibitor
            
            # Topoisomerase inhibitors
            'CN(C)C1=CC2=C(C=C1)C(=O)C3=C(C2=O)C=CC=C3O',       # Doxorubicin-like
            'COC1=CC2=C(C=C1)C(=CN2)C(=O)N3CCN(CC3)C',          # Topoisomerase
            
            # Alkylating agents
            'ClCCN(CCCl)N1C=NC2=C1C=CC(=C2)C(=O)N',             # Mechlorethamine-like
            'CC(C)C1=NC=C(N1)C2=CC=C(C=C2)N3CCOCC3',            # Alkylating
            
            # Antimetabolites
            'NC1=NC=NC2=C1N=CN2C3OC(CO)C(O)C3O',                # Adenine nucleoside
            'CC1=CN(C(=O)NC1=O)C2OC(C(C2O)O)CO',               # Pyrimidine analog
            'NC1=NC(=O)NC=C1C2=CC=CC=C2',                        # Pyrimidine
            
            # Natural products
            'CC1=C2C=C(C=CC2=CC(=C1)OC)OC3=C(C=CC(=C3)CC=C)O',  # Flavonoid-like
            'COC1=CC=C(C=C1)C2=COC3=CC=CC=C3C2=O',              # Flavonoid
            'CC(=O)OC1=CC=CC=C1C(=O)O',                          # Aspirin-like
            
            # Diverse scaffolds
            'C1=CC=C(C=C1)C2=CC=C(C=C2)C3=CC=CC=C3',           # Terphenyl
            'C1=CC2=C(C=C1)N=C(N2)C3=CC=CC=C3',                # Benzimidazole
            'CC(C)(C)C1=CC=C(C=C1)S(=O)(=O)NC2=CC=CC=C2',      # Sulfonamide
            'COC1=CC=C(C=C1)N2C=NC=N2',                         # Imidazole
            'CC1=CC=C(C=C1)C(=O)NC2=CC=C(C=C2)F',              # Amide
            
            # Additional diverse molecules
            'CN1CCN(CC1)C2=NC=NC3=C2SC=C3',                     # Thieno[2,3-d]pyrimidine
            'CC(C)OC(=O)C1=CC=CC=C1C(=O)OC(C)C',               # Diester
            'C1=CC=C2C(=C1)C=CC(=N2)C3=CC=CC=C3',              # Quinoline
            'COC1=CC=C(C=C1)C=CC(=O)C2=CC=C(C=C2)O',           # Chalcone
        ]
        
        # Extend with more SMILES - make this much more diverse
        additional_smiles = []
        base_smiles = ['CCO', 'c1ccccc1', 'CC(C)C', 'CCCCCCCC', 'c1ccc2ccccc2c1']
        
        # Generate many more variants
        for base in base_smiles:
            for i in range(50):  # Increase variants
                # Simple variations
                if base == 'CCO':
                    variations = [
                        'CCCO', 'CCCCO', 'CC(C)O', 'CC(O)C', 'CCC(O)CC', 'CC(C)(C)O', 'CCCCCO', 'CC(C)CO', 
                        'CC(CC)O', 'CCC(C)O', 'CCCCCCO', 'CC(C)CCCO', 'CCC(C)(C)O', 'CC(O)CC', 'CCCC(O)C',
                        'CCO[CH2]', 'CC[NH2]', 'CC(O)C(C)C', 'CCC(O)CCC', 'CC(C)C(O)C',
                        'CCON', 'CC(O)N', 'CCS', 'CC(O)S', 'CCF', 'CC(O)F', 'CCCl', 'CC(O)Cl'
                    ]
                elif base == 'c1ccccc1':
                    variations = [
                        'Cc1ccccc1', 'c1ccc(C)cc1', 'c1ccc(O)cc1', 'c1ccc(N)cc1', 'c1ccc(F)cc1', 'CCc1ccccc1', 
                        'c1ccc(CC)cc1', 'c1ccc(Cl)cc1', 'c1ccc(Br)cc1', 'COc1ccccc1', 'c1ccc(S)cc1', 'c1ccc(CF3)cc1',
                        'c1ccc(NO2)cc1', 'c1ccc(CN)cc1', 'c1ccc(CO)cc1', 'c1ccc(CCO)cc1', 'c1ccc(CCC)cc1',
                        'Nc1ccc(C)cc1', 'Oc1ccc(O)cc1', 'Fc1ccc(F)cc1', 'Clc1ccc(Cl)cc1', 'COc1ccc(OC)cc1',
                        'c1ccc2[nH]c3ccccc3c2c1', 'c1ccc2c(c1)ccc1ccccc12', 'c1cc2ccccc2cc1'
                    ]
                elif base == 'CCCCCCCC':
                    variations = [
                        'CCCCCCCCC', 'CCCCCCCCCC', 'CC(C)CCCCCC', 'CCC(C)CCCCC', 'CCCC(C)CCCC', 'CCCCC(C)CCC',
                        'CCCCCC(C)CC', 'CCCCCCC(C)C', 'CC(C)(C)CCCCC', 'CCC(C)(C)CCCC', 'CCCCCCCCCCCC',
                        'CC(C)CC(C)CCC', 'CCC(C)CC(C)CC', 'CCCCC=CCCC', 'CCCC=CCCCC', 'CCC=CCCCCC'
                    ]
                else:
                    continue
                    
                if i < len(variations):
                    additional_smiles.append(variations[i])
        
        # Add some realistic drug-like SMILES
        realistic_drugs = [
            'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O',  # Ibuprofen
            'CC(=O)OC1=CC=CC=C1C(=O)O',       # Aspirin
            'CC(C)NCC(C1=CC=C(O)C=C1)O',      # Albuterol-like
            'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',   # Caffeine
            'CCC1=C(C(=CC=C1)C)NC(=O)C',      # Lidocaine-like
            'CC(C)(C)NCC(C1=CC(=C(C=C1)O)CO)O', # Salbutamol-like
            'CCCN(CCC)C(=O)C1=C(C=CC=C1Cl)Cl',  # Diclofenac-like
            'COC1=C(C=C2C(=C1)C=CC(=O)O2)C=CCO', # Coumarin derivative
        ]
        additional_smiles.extend(realistic_drugs)
        
        self.drug_smiles.extend(additional_smiles)
        
        # Remove duplicates
        self.drug_smiles = list(set(self.drug_smiles))
        
        logger.info(f"‚úÖ Initialized with {len(self.cell_lines)} cell lines and {len(self.drug_smiles)} drug SMILES")
    
    def generate_realistic_dataset(self, n_samples=15000):
        """Generate realistic GDSC-like dataset with chemical and biological relationships"""
        
        logger.info(f"üß¨ GENERATING REALISTIC GDSC DATASET ({n_samples:,} samples)")
        
        data = []
        cell_line_names = list(self.cell_lines.keys())
        
        for i in range(n_samples):
            # Random cell line and drug
            cell_line = np.random.choice(cell_line_names)
            drug_smiles = np.random.choice(self.drug_smiles)
            tissue_type = self.cell_lines[cell_line]
            
            # Generate pIC50 based on realistic relationships
            pIC50 = self._generate_realistic_pic50(drug_smiles, cell_line, tissue_type)
            
            data.append({
                'SMILES': drug_smiles,
                'CELL_LINE_NAME': cell_line,
                'tissue_type': tissue_type,
                'pIC50': pIC50
            })
        
        df = pd.DataFrame(data)
        
        # Add some noise and realistic distributions
        df['pIC50'] += np.random.normal(0, 0.3, len(df))  # Experimental noise
        df['pIC50'] = np.clip(df['pIC50'], 3.5, 9.5)      # Realistic range
        
        # Remove duplicates
        df = df.drop_duplicates(subset=['SMILES', 'CELL_LINE_NAME'], keep='first')
        
        logger.info(f"‚úÖ Generated dataset: {len(df):,} unique combinations")
        logger.info(f"   Unique SMILES: {df['SMILES'].nunique():,}")
        logger.info(f"   Unique cell lines: {df['CELL_LINE_NAME'].nunique()}")
        logger.info(f"   pIC50 range: {df['pIC50'].min():.2f} - {df['pIC50'].max():.2f}")
        
        return df
    
    def _generate_realistic_pic50(self, smiles, cell_line, tissue_type):
        """Generate realistic pIC50 values based on drug-cell line relationships"""
        
        # Base activity from drug properties
        if RDKIT_AVAILABLE:
            mol = Chem.MolFromSmiles(smiles)
            if mol is not None:
                mw = Descriptors.MolWt(mol)
                logp = Descriptors.MolLogP(mol)
                hbd = Descriptors.NumHDonors(mol)
                hba = Descriptors.NumHAcceptors(mol)
                tpsa = Descriptors.TPSA(mol)
                
                # Drug-like score influences activity
                lipinski_violations = sum([
                    mw > 500,
                    logp > 5,
                    hbd > 5,
                    hba > 10
                ])
                
                base_activity = 6.0  # Average pIC50
                
                # Molecular weight effect
                if mw < 200:
                    base_activity -= 0.5  # Too small
                elif mw > 600:
                    base_activity -= 0.8  # Too large
                    
                # LogP effect (optimal around 2-3)
                if logp < 0:
                    base_activity -= 0.3  # Too hydrophilic
                elif logp > 5:
                    base_activity -= 0.6  # Too lipophilic
                elif 1 < logp < 4:
                    base_activity += 0.2  # Optimal range
                
                # TPSA effect
                if tpsa > 140:
                    base_activity -= 0.4  # Poor permeability
                elif 60 < tpsa < 90:
                    base_activity += 0.1  # Good permeability
                
                # Lipinski violations penalty
                base_activity -= lipinski_violations * 0.3
                
            else:
                base_activity = 5.5  # Default for invalid SMILES
        else:
            # Simplified scoring without RDKit
            base_activity = 6.0 + np.random.normal(0, 0.5)
            
            # Simple SMILES-based rules
            if len(smiles) > 50:
                base_activity -= 0.3  # Complex molecules
            if 'N' in smiles and 'O' in smiles:
                base_activity += 0.2  # Potential for H-bonding
            if smiles.count('=') > 3:
                base_activity += 0.1  # Aromatic systems
        
        # Tissue-specific effects
        tissue_effects = {
            'lung': np.random.normal(0, 0.4),
            'breast': np.random.normal(0.1, 0.3),  # Slightly more sensitive
            'colon': np.random.normal(-0.1, 0.4),  # Slightly resistant
            'skin': np.random.normal(0.2, 0.5),    # Variable
            'prostate': np.random.normal(0, 0.3),
            'liver': np.random.normal(-0.2, 0.4),  # Metabolically active
            'blood': np.random.normal(0.3, 0.4),   # Often more sensitive
            'brain': np.random.normal(-0.3, 0.3),  # Blood-brain barrier
            'ovarian': np.random.normal(0.1, 0.4)
        }
        
        tissue_effect = tissue_effects.get(tissue_type, np.random.normal(0, 0.3))
        
        # Cell line specific effects
        np.random.seed(hash(cell_line + smiles) % (2**32))
        cell_line_effect = np.random.normal(0, 0.4)
        
        final_pic50 = base_activity + tissue_effect + cell_line_effect
        
        # Add some extreme responders (very sensitive or very resistant)
        if np.random.random() < 0.05:  # 5% extreme responders
            extreme_effect = np.random.choice([-2, 2]) * np.random.uniform(0.5, 1.5)
            final_pic50 += extreme_effect
        
        return final_pic50

class GenomicFeatureGenerator:
    """Generate realistic genomic features for cell lines"""
    
    def __init__(self):
        # Cancer genes with realistic mutation frequencies
        self.mutation_genes = [
            'TP53', 'KRAS', 'PIK3CA', 'PTEN', 'BRAF', 'EGFR', 'MYC', 'RB1',
            'APC', 'BRCA1', 'BRCA2', 'NRAS', 'HRAS', 'CDK4', 'CDKN2A',
            'VHL', 'ARID1A', 'SMAD4', 'FBXW7', 'ATM'  # 20 genes
        ]
        
        # CNV genes
        self.cnv_genes = ['MYC', 'EGFR', 'HER2', 'CCND1', 'MDM2']  # 5 genes
        
        # Tissue types for one-hot encoding
        self.tissue_types = ['lung', 'breast', 'colon', 'skin', 'prostate', 'liver', 'blood', 'brain', 'ovarian']
    
    def generate_features(self, cell_lines, tissue_types):
        """Generate genomic features for cell lines"""
        
        logger.info(f"üß¨ Generating genomic features for {len(cell_lines)} cell lines")
        
        features = []
        
        for cell_line, tissue in zip(cell_lines, tissue_types):
            feature_vector = []
            
            # 1. Mutation features (20 genes = 20 features)
            np.random.seed(hash(cell_line) % (2**32))
            for gene in self.mutation_genes:
                # Realistic mutation frequencies
                if gene == 'TP53':
                    mut_prob = 0.5
                elif gene in ['KRAS', 'PIK3CA']:
                    mut_prob = 0.3
                elif gene in ['PTEN', 'BRAF']:
                    mut_prob = 0.2
                else:
                    mut_prob = 0.1
                
                # Tissue-specific modulation
                if tissue == 'lung' and gene in ['KRAS', 'EGFR', 'TP53']:
                    mut_prob *= 1.5
                elif tissue == 'breast' and gene in ['PIK3CA', 'BRCA1', 'BRCA2']:
                    mut_prob *= 1.8
                elif tissue == 'colon' and gene in ['APC', 'KRAS']:
                    mut_prob *= 2.0
                elif tissue == 'skin' and gene == 'BRAF':
                    mut_prob *= 3.0
                
                mut_status = int(np.random.random() < min(mut_prob, 0.8))
                feature_vector.append(mut_status)
            
            # 2. CNV features (5 genes √ó 3 states = 15 features)
            for gene in self.cnv_genes:
                # CNV states: loss (-1), neutral (0), gain (1)
                cnv_probs = [0.1, 0.7, 0.2]  # [loss, neutral, gain]
                cnv_state = np.random.choice([-1, 0, 1], p=cnv_probs)
                
                # One-hot encode: [loss, neutral, gain]
                cnv_onehot = [0, 0, 0]
                cnv_onehot[cnv_state + 1] = 1
                feature_vector.extend(cnv_onehot)
            
            # 3. Tissue type one-hot (9 features)
            tissue_onehot = [0] * len(self.tissue_types)
            if tissue in self.tissue_types:
                tissue_idx = self.tissue_types.index(tissue)
                tissue_onehot[tissue_idx] = 1
            feature_vector.extend(tissue_onehot)
            
            features.append(feature_vector)
        
        features = np.array(features, dtype=np.float32)
        total_expected = 20 + 15 + 9  # mutations + cnv + tissue = 44
        
        logger.info(f"‚úÖ Generated genomic features: {features.shape}")
        logger.info(f"   Expected: {len(cell_lines)} √ó {total_expected} features")
        logger.info(f"   Mutation genes: 20, CNV genes: 15, Tissue types: 9")
        
        return features

class ChemBERTaCytotoxModel(nn.Module):
    """ChemBERTa encoder with cytotoxicity prediction head"""
    
    def __init__(self, molecular_dim, genomic_dim=44, hidden_dim=512):
        super().__init__()
        
        # Use actual ChemBERTa output dimension
        self.molecular_dim = molecular_dim
        self.genomic_dim = genomic_dim
        
        # Genomic encoder
        self.genomic_encoder = nn.Sequential(
            nn.Linear(genomic_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Fusion layers
        combined_dim = molecular_dim + 128  # molecular_dim + 128 genomic features
        self.fusion = nn.Sequential(
            nn.LayerNorm(combined_dim),
            nn.Dropout(0.3),
            nn.Linear(combined_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, 256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, 1)
        )
        
        logger.info(f"‚úÖ Model architecture: {molecular_dim} + {genomic_dim} ‚Üí {combined_dim} ‚Üí {hidden_dim} ‚Üí 1")
    
    def forward(self, molecular_features, genomic_features):
        """Forward pass"""
        
        # Encode genomic features
        genomic_encoded = self.genomic_encoder(genomic_features)
        
        # Combine molecular and genomic features
        combined = torch.cat([molecular_features, genomic_encoded], dim=1)
        
        # Predict pIC50
        prediction = self.fusion(combined)
        
        return prediction

def train_chemberta_cytotox_model():
    """Train ChemBERTa cytotoxicity model locally"""
    
    logger.info("üéØ LOCAL CHEMBERTA CYTOTOXICITY TRAINING")
    logger.info("=" * 80)
    logger.info("TARGET: R¬≤ > 0.7 using ChemBERTa + realistic synthetic data")
    logger.info("STRATEGY: Generate GDSC-like data + ChemBERTa encoder + deep NN")
    logger.info("=" * 80)
    
    # Check if CUDA is available
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"üñ•Ô∏è Using device: {device}")
    
    # 1. GENERATE REALISTIC DATASET
    logger.info("1Ô∏è‚É£ GENERATING REALISTIC DATASET")
    
    data_generator = SyntheticGDSCDataGenerator()
    dataset = data_generator.generate_realistic_dataset(n_samples=50000)
    
    # 2. SETUP CHEMBERTA ENCODER
    logger.info("2Ô∏è‚É£ SETTING UP CHEMBERTA ENCODER")
    
    try:
        model_name = "DeepChem/ChemBERTa-77M-MLM"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        chemberta_model = AutoModel.from_pretrained(model_name)
        chemberta_model.to(device)
        chemberta_model.eval()
        
        # Freeze ChemBERTa parameters
        for param in chemberta_model.parameters():
            param.requires_grad = False
        
        logger.info(f"‚úÖ ChemBERTa loaded and frozen: {model_name}")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load ChemBERTa: {e}")
        logger.info("üí° Please install transformers: pip install transformers")
        return None
    
    # 3. ENCODE MOLECULAR FEATURES
    logger.info("3Ô∏è‚É£ ENCODING MOLECULAR FEATURES")
    
    unique_smiles = dataset['SMILES'].unique()
    molecular_features_dict = {}
    
    batch_size = 32
    logger.info(f"üß¨ Encoding {len(unique_smiles):,} unique SMILES...")
    
    for i in range(0, len(unique_smiles), batch_size):
        batch_smiles = unique_smiles[i:i+batch_size]
        
        # Tokenize
        inputs = tokenizer(
            list(batch_smiles),
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=512
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Get ChemBERTa embeddings
        with torch.no_grad():
            outputs = chemberta_model(**inputs)
            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        
        # Store embeddings
        for smiles, embedding in zip(batch_smiles, cls_embeddings):
            molecular_features_dict[smiles] = embedding
        
        if (i // batch_size + 1) % 10 == 0:
            logger.info(f"   Processed {i + batch_size:,} / {len(unique_smiles):,} SMILES")
    
    molecular_dim = list(molecular_features_dict.values())[0].shape[0]
    logger.info(f"‚úÖ Molecular encoding complete: {molecular_dim}-dim features")
    
    # 4. GENERATE GENOMIC FEATURES
    logger.info("4Ô∏è‚É£ GENERATING GENOMIC FEATURES")
    
    genomic_generator = GenomicFeatureGenerator()
    genomic_features = genomic_generator.generate_features(
        dataset['CELL_LINE_NAME'].tolist(),
        dataset['tissue_type'].tolist()
    )
    
    # 5. PREPARE TRAINING DATA
    logger.info("5Ô∏è‚É£ PREPARING TRAINING DATA")
    
    # Get molecular features for all samples
    X_molecular = np.array([molecular_features_dict[smiles] for smiles in dataset['SMILES']])
    X_genomic = genomic_features
    y = dataset['pIC50'].values
    
    logger.info(f"üìä Training data prepared:")
    logger.info(f"   Molecular: {X_molecular.shape}")
    logger.info(f"   Genomic: {X_genomic.shape}")
    logger.info(f"   Targets: {y.shape}")
    
    # 6. CREATE SPLITS
    logger.info("6Ô∏è‚É£ CREATING DATA SPLITS")
    
    # 80/10/10 split
    X_mol_temp, X_mol_test, X_gen_temp, X_gen_test, y_temp, y_test = train_test_split(
        X_molecular, X_genomic, y, test_size=0.1, random_state=42, stratify=None
    )
    
    X_mol_train, X_mol_val, X_gen_train, X_gen_val, y_train, y_val = train_test_split(
        X_mol_temp, X_gen_temp, y_temp, test_size=0.111, random_state=42  # 0.111 * 0.9 ‚âà 0.1
    )
    
    logger.info(f"‚úÖ Data splits: {len(y_train)}/{len(y_val)}/{len(y_test)}")
    
    # 7. SCALE FEATURES
    logger.info("7Ô∏è‚É£ SCALING FEATURES")
    
    # Scale molecular features
    mol_scaler = StandardScaler()
    X_mol_train_scaled = mol_scaler.fit_transform(X_mol_train)
    X_mol_val_scaled = mol_scaler.transform(X_mol_val)
    X_mol_test_scaled = mol_scaler.transform(X_mol_test)
    
    # Scale genomic features
    gen_scaler = StandardScaler()
    X_gen_train_scaled = gen_scaler.fit_transform(X_gen_train)
    X_gen_val_scaled = gen_scaler.transform(X_gen_val)
    X_gen_test_scaled = gen_scaler.transform(X_gen_test)
    
    logger.info("‚úÖ Feature scaling complete")
    
    # 8. CREATE MODEL
    logger.info("8Ô∏è‚É£ CREATING CYTOTOXICITY MODEL")
    
    model = ChemBERTaCytotoxModel(
        molecular_dim=molecular_dim,
        genomic_dim=X_genomic.shape[1],
        hidden_dim=512
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"‚úÖ Model created: {total_params:,} total, {trainable_params:,} trainable params")
    
    # 9. TRAINING SETUP
    logger.info("9Ô∏è‚É£ TRAINING SETUP")
    
    criterion = nn.HuberLoss(delta=1.0)
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.7, patience=10
    )
    
    # Convert to tensors
    X_mol_train_t = torch.FloatTensor(X_mol_train_scaled).to(device)
    X_gen_train_t = torch.FloatTensor(X_gen_train_scaled).to(device)  
    y_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(device)
    
    X_mol_val_t = torch.FloatTensor(X_mol_val_scaled).to(device)
    X_gen_val_t = torch.FloatTensor(X_gen_val_scaled).to(device)
    y_val_t = torch.FloatTensor(y_val).unsqueeze(1).to(device)
    
    logger.info("‚úÖ Training tensors prepared")
    
    # 10. TRAINING LOOP
    logger.info("üîü TRAINING MODEL")
    
    best_val_r2 = -np.inf
    best_model_state = None
    patience_counter = 0
    patience = 25
    
    for epoch in range(200):  # Reasonable training time
        # Training
        model.train()
        optimizer.zero_grad()
        
        predictions = model(X_mol_train_t, X_gen_train_t)
        loss = criterion(predictions, y_train_t)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        # Validation every 5 epochs
        if (epoch + 1) % 5 == 0:
            model.eval()
            with torch.no_grad():
                val_predictions = model(X_mol_val_t, X_gen_val_t)
                val_r2 = r2_score(y_val, val_predictions.cpu().numpy())
                val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions.cpu().numpy()))
                val_pearson, _ = pearsonr(y_val, val_predictions.cpu().numpy().flatten())
                
                logger.info(f"Epoch {epoch+1:3d}: Loss={loss:.4f}, Val R¬≤={val_r2:.4f}, RMSE={val_rmse:.4f}, r={val_pearson:.4f}")
                
                if val_r2 > best_val_r2:
                    best_val_r2 = val_r2
                    best_model_state = model.state_dict().copy()
                    patience_counter = 0
                    
                    if val_r2 >= 0.7:
                        logger.info(f"üéâ TARGET ACHIEVED! Val R¬≤ = {val_r2:.4f} ‚â• 0.7")
                        break
                else:
                    patience_counter += 1
                
                scheduler.step(val_r2)
                
                if patience_counter >= patience:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
    
    # 11. FINAL EVALUATION
    logger.info("1Ô∏è‚É£1Ô∏è‚É£ FINAL EVALUATION")
    
    # Load best model
    if best_model_state:
        model.load_state_dict(best_model_state)
    
    model.eval()
    with torch.no_grad():
        # Test evaluation
        X_mol_test_t = torch.FloatTensor(X_mol_test_scaled).to(device)
        X_gen_test_t = torch.FloatTensor(X_gen_test_scaled).to(device)
        test_predictions = model(X_mol_test_t, X_gen_test_t)
        
        test_r2 = r2_score(y_test, test_predictions.cpu().numpy())
        test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions.cpu().numpy()))
        test_mae = mean_absolute_error(y_test, test_predictions.cpu().numpy())
        test_pearson, _ = pearsonr(y_test, test_predictions.cpu().numpy().flatten())
        test_spearman, _ = spearmanr(y_test, test_predictions.cpu().numpy().flatten())
    
    # 12. SAVE MODEL
    logger.info("1Ô∏è‚É£2Ô∏è‚É£ SAVING MODEL")
    
    model_save_path = "/app/models/chemberta_cytotox_model.pth"
    os.makedirs("/app/models", exist_ok=True)
    
    save_dict = {
        'model_state_dict': best_model_state,
        'model_config': {
            'molecular_dim': molecular_dim,
            'genomic_dim': X_genomic.shape[1], 
            'hidden_dim': 512,
            'architecture': 'chemberta_cytotox_local'
        },
        'training_results': {
            'best_val_r2': float(best_val_r2),
            'test_r2': float(test_r2),
            'test_rmse': float(test_rmse),
            'test_mae': float(test_mae),
            'test_pearson': float(test_pearson),
            'test_spearman': float(test_spearman),
            'target_achieved': best_val_r2 >= 0.7
        },
        'data_info': {
            'total_samples': len(dataset),
            'train_samples': len(y_train),
            'val_samples': len(y_val),
            'test_samples': len(y_test),
            'unique_smiles': len(unique_smiles),
            'molecular_dim': molecular_dim,
            'genomic_dim': X_genomic.shape[1]
        },
        'scalers': {
            'molecular_scaler': mol_scaler,
            'genomic_scaler': gen_scaler
        }
    }
    
    torch.save(save_dict, model_save_path)
    
    # 13. RESULTS SUMMARY
    logger.info("üèÅ CHEMBERTA CYTOTOXICITY TRAINING COMPLETE")
    logger.info("=" * 80)
    logger.info(f"üèÜ VALIDATION R¬≤: {best_val_r2:.4f}")
    logger.info(f"üß™ TEST R¬≤: {test_r2:.4f}")
    logger.info(f"üìä TEST RMSE: {test_rmse:.4f}")
    logger.info(f"üìä TEST MAE: {test_mae:.4f}")
    logger.info(f"üìä TEST Pearson r: {test_pearson:.4f}")
    logger.info(f"üìä TEST Spearman œÅ: {test_spearman:.4f}")
    logger.info(f"üéØ TARGET (R¬≤ ‚â• 0.7): {'‚úÖ ACHIEVED' if best_val_r2 >= 0.7 else 'üìà IN PROGRESS'}")
    logger.info(f"üì¶ Model saved: {model_save_path}")
    logger.info("=" * 80)
    
    return {
        'val_r2': best_val_r2,
        'test_r2': test_r2,
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'test_pearson': test_pearson,
        'test_spearman': test_spearman,
        'target_achieved': best_val_r2 >= 0.7,
        'model_path': model_save_path,
        'data_samples': len(dataset),
        'molecular_dim': molecular_dim,
        'genomic_dim': X_genomic.shape[1],
        'approach': 'local_chemberta_cytotox'
    }

if __name__ == "__main__":
    logger.info("üß¨ LOCAL CHEMBERTA CYTOTOXICITY TRAINING")
    logger.info("üéØ TARGET: R¬≤ > 0.7 with ChemBERTa + synthetic GDSC data")
    
    result = train_chemberta_cytotox_model()
    
    if result:
        logger.info("üéâ TRAINING COMPLETED!")
        logger.info(f"üìä Results: {result}")
        
        if result.get('target_achieved'):
            logger.info("üèÜ SUCCESS: R¬≤ > 0.7 TARGET ACHIEVED!")
        else:
            logger.info(f"üìà Progress: R¬≤ = {result.get('val_r2', 0):.4f}")
    else:
        logger.error("‚ùå Training failed!")