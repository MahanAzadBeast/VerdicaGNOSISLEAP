"""
Clinical Trial Dataset Builder - Main Pipeline
Complete end-to-end dataset creation for commercial use
"""

import logging
import pandas as pd
import json
from pathlib import Path
from datetime import datetime

from src.data_collectors.clinicaltrials_gov import ClinicalTrialsCollector
from src.data_collectors.pubchem_mapper import SMILESMapper
from src.processors.outcome_labeler import OutcomeLabeler
from src.processors.feature_engineer import FeatureEngineer
from src.processors.quality_control import QualityController
import config

def setup_logging():
    """Setup logging configuration"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"dataset_creation_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

def save_pipeline_metadata(quality_stats: dict, logger):
    """Save metadata about the dataset creation process"""
    
    metadata = {
        'dataset_info': {
            'creation_date': datetime.now().isoformat(),
            'total_records': quality_stats['final_records'],
            'success_rate': quality_stats['balance_stats']['overall_success_rate'],
            'data_sources': ['ClinicalTrials.gov', 'PubChem', 'ChEMBL'],
            'date_range': f"{config.START_DATE} to {config.END_DATE}",
            'target_records': config.TARGET_RECORDS
        },
        'quality_metrics': {
            'initial_records': quality_stats['initial_records'],
            'final_records': quality_stats['final_records'],
            'removal_rate': quality_stats['removal_rate'],
            'smiles_mapping_success': 'calculated_during_pipeline'  # Will be updated
        },
        'data_splits': {
            'train_size': quality_stats['train_size'],
            'val_size': quality_stats['val_size'],
            'test_size': quality_stats['test_size']
        },
        'feature_info': {
            'total_features': quality_stats['total_features'],
            'feature_types': 'molecular, sponsor, disease, protocol, temporal, text'
        },
        'phase_distribution': quality_stats['balance_stats']['phase_balance'],
        'disease_distribution': quality_stats['balance_stats']['disease_balance']
    }
    
    # Save metadata
    metadata_path = "data/final/dataset_metadata.json"
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2, default=str)
    
    logger.info(f"Dataset metadata saved to {metadata_path}")
    
    return metadata

def print_final_summary(metadata: dict, logger):
    """Print final dataset summary"""
    
    logger.info("=" * 60)
    logger.info("CLINICAL TRIAL DATASET CREATION COMPLETED!")
    logger.info("=" * 60)
    
    dataset_info = metadata['dataset_info']
    quality_metrics = metadata['quality_metrics']
    
    logger.info(f"üìä Final Dataset Size: {dataset_info['total_records']:,} clinical trials")
    logger.info(f"üìà Overall Success Rate: {dataset_info['success_rate']:.1f}%")
    logger.info(f"üß™ Features: {metadata['feature_info']['total_features']} total features")
    
    logger.info(f"\nüìã Data Splits:")
    logger.info(f"  ‚Ä¢ Training: {metadata['data_splits']['train_size']:,} trials")
    logger.info(f"  ‚Ä¢ Validation: {metadata['data_splits']['val_size']:,} trials")
    logger.info(f"  ‚Ä¢ Test: {metadata['data_splits']['test_size']:,} trials")
    
    logger.info(f"\nüè• Phase Distribution:")
    for phase, stats in metadata['phase_distribution'].items():
        logger.info(f"  ‚Ä¢ {phase}: {stats['total']:,} trials ({stats['success_rate']:.1f}% success)")
    
    logger.info(f"\nüéØ Top Disease Categories:")
    sorted_diseases = sorted(metadata['disease_distribution'].items(), 
                           key=lambda x: x[1]['total'], reverse=True)
    for disease, stats in sorted_diseases[:5]:
        logger.info(f"  ‚Ä¢ {disease}: {stats['total']:,} trials ({stats['success_rate']:.1f}% success)")
    
    logger.info(f"\nüìÅ Output Files:")
    logger.info(f"  ‚Ä¢ Complete dataset: data/final/complete_dataset.parquet")
    logger.info(f"  ‚Ä¢ Training set: data/final/train_set.parquet")
    logger.info(f"  ‚Ä¢ Validation set: data/final/val_set.parquet")
    logger.info(f"  ‚Ä¢ Test set: data/final/test_set.parquet")
    logger.info(f"  ‚Ä¢ Metadata: data/final/dataset_metadata.json")
    
    logger.info(f"\n‚úÖ Dataset ready for machine learning!")
    logger.info(f"‚úÖ All data is legally cleared for commercial use!")
    logger.info("=" * 60)

def main():
    """Main pipeline execution"""
    
    # Setup
    logger = setup_logging()
    logger.info("üöÄ Starting Clinical Trial Dataset Creation Pipeline...")
    
    # Create output directories
    Path("data/raw").mkdir(parents=True, exist_ok=True)
    Path("data/processed").mkdir(parents=True, exist_ok=True)
    Path("data/final").mkdir(parents=True, exist_ok=True)
    
    try:
        # Step 1: Collect clinical trials data
        logger.info("\n" + "="*50)
        logger.info("STEP 1: COLLECTING CLINICAL TRIALS DATA")
        logger.info("="*50)
        
        collector = ClinicalTrialsCollector()
        raw_trials = collector.collect_all_trials(max_records=config.TARGET_RECORDS)
        logger.info(f"‚úÖ Collected {len(raw_trials):,} clinical trials")
        
        # Step 2: Map drugs to SMILES
        logger.info("\n" + "="*50)
        logger.info("STEP 2: MAPPING DRUGS TO SMILES")
        logger.info("="*50)
        
        mapper = SMILESMapper()
        trials_with_smiles = mapper.map_trials_to_smiles(raw_trials)
        smiles_success_count = len(trials_with_smiles[trials_with_smiles['smiles'].notna()])
        smiles_success_rate = smiles_success_count / len(trials_with_smiles) * 100
        logger.info(f"‚úÖ Mapped {smiles_success_count:,} trials to SMILES ({smiles_success_rate:.1f}% success)")
        
        # Step 3: Create outcome labels
        logger.info("\n" + "="*50)
        logger.info("STEP 3: CREATING OUTCOME LABELS")
        logger.info("="*50)
        
        labeler = OutcomeLabeler()
        trials_with_outcomes = labeler.label_outcomes(trials_with_smiles)
        logger.info(f"‚úÖ Created outcome labels for {len(trials_with_outcomes):,} trials")
        
        # Step 4: Engineer features
        logger.info("\n" + "="*50)
        logger.info("STEP 4: ENGINEERING FEATURES")
        logger.info("="*50)
        
        engineer = FeatureEngineer()
        trials_with_features = engineer.engineer_all_features(trials_with_outcomes)
        logger.info(f"‚úÖ Engineered {len(trials_with_features.columns)} total features")
        
        # Step 5: Quality control and final preparation
        logger.info("\n" + "="*50)
        logger.info("STEP 5: QUALITY CONTROL & FINAL PREPARATION")
        logger.info("="*50)
        
        controller = QualityController()
        final_dataset, quality_stats = controller.apply_quality_control(trials_with_features)
        logger.info(f"‚úÖ Final dataset: {len(final_dataset):,} records with {len(final_dataset.columns)} features")
        
        # Step 6: Save metadata and summary
        logger.info("\n" + "="*50)
        logger.info("STEP 6: SAVING METADATA & SUMMARY")
        logger.info("="*50)
        
        # Update quality stats with SMILES success rate
        quality_stats['smiles_mapping_success_rate'] = smiles_success_rate
        
        metadata = save_pipeline_metadata(quality_stats, logger)
        print_final_summary(metadata, logger)
        
        logger.info("üéâ Dataset creation pipeline completed successfully!")
        
        return final_dataset, metadata
        
    except Exception as e:
        logger.error(f"‚ùå Pipeline failed with error: {str(e)}")
        logger.error(f"Error type: {type(e).__name__}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise

if __name__ == "__main__":
    dataset, metadata = main()