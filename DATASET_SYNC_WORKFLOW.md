# ğŸ”„ GNOSIS Dataset Sync Workflow
## Modal â†’ S3 Transfer with Audit-Ready Lineage

### ğŸ¯ **Objective**
Sync curated datasets, deterministic splits, and eval reports from Modal to S3 with checksums and metadata for reproducibility. Establish S3 as source of truth for both training and inference.

---

## ğŸ“‹ **Phase 1: Setup (One-time)**

### 1. Setup Modal AWS Secrets
```bash
cd /app/modal_training
python setup_aws_secrets.py
```
**Output**: Creates `aws-credentials` secret in Modal

### 2. Verify Modal Volumes
```bash
modal volume list
```
**Expected**: `veridica-datasets` and `veridica-models` volumes

---

## ğŸš€ **Phase 2: Execute Sync**

### 1. Run Modal â†’ S3 Sync Job
```bash
cd /app/modal_training
modal run sync_datasets_to_s3.py
```

**What it syncs:**
- âœ… **Curated datasets** (processed ChEMBL, BindingDB, GDSC)
- âœ… **Deterministic splits** (train/val/test)
- âœ… **Evaluation reports** (model performance metrics)
- âœ… **SHA256 checksums** (integrity verification)
- âœ… **Metadata files** (provenance tracking)

**S3 Structure Created:**
```
s3://veridicabatabase/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ chembl/v32/
â”‚   â”‚   â”œâ”€â”€ *.csv (curated data)
â”‚   â”‚   â””â”€â”€ metadata.json (checksums + provenance)
â”‚   â”œâ”€â”€ bindingdb/v2024/
â”‚   â”‚   â”œâ”€â”€ *.csv (curated data)
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”œâ”€â”€ gdsc/v1.0/
â”‚   â”‚   â”œâ”€â”€ *.csv (curated data)
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â””â”€â”€ _sync_reports/
â”‚       â””â”€â”€ 20250821_120000_sync_report.json
â””â”€â”€ models/
    â”œâ”€â”€ gnosis-i/1.0.0/reports/
    â””â”€â”€ gnosis-ii/1.0.0/reports/
```

### 2. Update Model Registry
```bash
cd /app/backend
python model_registry/update_datasets.py
```

**What it does:**
- âœ… Reads S3 sync reports
- âœ… Updates MongoDB with real dataset metadata
- âœ… Updates YAML manifests with file counts & checksums
- âœ… Verifies S3 integrity

---

## ğŸ“Š **Phase 3: Verification**

### 1. Check Registry Status
```bash
curl -s "http://localhost:8001/api/registry/stats" | jq '.'
```

### 2. Verify Dataset Lineage
```bash
curl -s "http://localhost:8001/api/registry/discover/summary" | jq '.datasets'
```

### 3. Test S3 Access
```bash
# List synced datasets
aws s3 ls s3://veridicabatabase/datasets/ --recursive

# Download a metadata file
aws s3 cp s3://veridicabatabase/datasets/chembl/v32/metadata.json /tmp/
```

---

## âœ… **Expected Results**

**Before Sync:**
```json
{
  "models_count": 2,
  "datasets_count": 0,  // â† No real datasets
  "artifacts_count": 2
}
```

**After Sync:**
```json
{
  "models_count": 2,
  "datasets_count": 3,  // â† Real datasets registered
  "artifacts_count": 2,
  "total_dataset_size_mb": 750,
  "s3_objects": 15
}
```

**Audit Trail Created:**
1. **SHA256 checksums** for every file
2. **Sync timestamps** and provenance
3. **File-level metadata** (size, type, path)
4. **Model-dataset lineage** mapping
5. **Reproducible splits** with deterministic seeds

---

## ğŸ”„ **Ongoing Workflow**

### For New Datasets:
1. Process on Modal volumes
2. Run sync job: `modal run sync_datasets_to_s3.py`
3. Update registry: `python update_datasets.py`

### For Training/Inference:
- **Source of truth**: S3 datasets (not Modal volumes)
- **Verification**: Check SHA256 before training
- **Lineage**: Track which S3 dataset version was used

---

## ğŸ¯ **Benefits Achieved**

âœ… **Audit-ready lineage**: Every model â†’ dataset â†’ file mapping  
âœ… **Reproducible splits**: Deterministic train/val/test  
âœ… **Integrity verification**: SHA256 checksums  
âœ… **Source of truth**: S3 for both training & inference  
âœ… **Cost efficient**: Only curated data, no raw dumps  
âœ… **Scalable**: Supports 100+ datasets with metadata  

---

## ğŸš¨ **Troubleshooting**

**Modal volume not found:**
```bash
modal volume create veridica-datasets
modal volume create veridica-models
```

**AWS credentials error:**
```bash
# Re-run secret setup
python setup_aws_secrets.py
modal secret list  # Verify 'aws-credentials' exists
```

**No datasets found on Modal:**
```bash
# Check Modal volumes
modal run -c "ls -la /datasets/"
modal run -c "find /datasets/ -name '*.csv' | head -10"
```

**S3 sync incomplete:**
```bash
# Check sync report
aws s3 cp s3://veridicabatabase/datasets/_sync_reports/latest.json /tmp/
cat /tmp/latest.json | jq '.total_files'
```

---

## ğŸ“ˆ **Next Steps**

After successful sync:
1. âœ… **Models use S3 datasets** (not local/Modal)
2. âœ… **Training reproducibility** via checksums
3. âœ… **Ready for Phase 3**: New prediction modules
4. âœ… **Audit compliance** for regulatory submissions

**Ready to proceed with Gnosis III-V modules! ğŸš€**